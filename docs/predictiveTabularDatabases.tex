\documentclass[11pt]{article}


\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{epsfig, amsmath, amsfonts, soul, color}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{fullpage}
\usepackage{placeins}

\title{Predictive tabular databases for automatic machine learning}
\author{Patrick Shafto, Vikash K. Mansinghka...}

\begin{document}
\maketitle

\section{Predictive database}

Consists of a language for interfacing with domain-general multidimensional density approximator e.g, CrossCat!

\section{Queries}

\subsection{Core inference}

\begin{itemize}

\item \texttt{initialize($M_c, M_r, T, i$)} $\rightarrow \{ M_c, M_r, X_L, X_D \}$. $M_c$ is the metadata for columns including bidirectional mappings between labels and indices, the modeltype---`asymmetric\_beta\_bernoulli' for binary data, `symmetric\_dirichlet\_discrete' for closed categorical data, `pitmanyor\_atom' for open categorical data, `poisson\_gamma' for frequency data, and `normal\_inverse\_gamma' for numeric data. $M_r$ is the metadata for rows including bidirectional mappings between labels and indices. $T$ is a table indicating the values for a collection of variables, and $i$ is an initialization (together, apart, or from\_the\_prior). It returns $X_L$ and $X_D$. $X_L$ is the latent state, including all information necessary for prediction. $X_D$ is a mapping between the latent state and the table $T$. 

\item \texttt{analyze($M_c, T, X_L, X_D, kernel\_list, n\_steps, c, r, max\_iterations, max\_time$)} $\rightarrow \{X'_L, X'_D\}$. 
$kernel\_list$ contains a list of the kernels to be run. This list may contain one or more of the following: `column\_partition\_hyperparameter', `column\_partition\_assignments', `component\_hyperparameters', `row\_partition\_hyperparameters', and/or `row\_partition\_assignments'.
$n\_steps$ is the number of times the $kernel\_list$ will be run. $c$ is an array that specifies which columns on which to run the kernels, by indices or `all'. This applies only to column kernels. $r$ is an array that specifies which rows on which to run the kernels and applies only to row kernels.
$max\_iterations$ is the maximum number of desired iterations and $max\_time$ is the maximum wall clock time before returning a state. \texttt{Analyze} returns a new latent state, $X'_L$, and a corresponding mapping between the state and the table, $X'_D$, based on the $min(max\_iterations, max\_time)$. 

\end{itemize}

\subsection{Data manipulation}

\subsection{Prediction}

\begin{itemize}

% SimplePredictiveSample
\item \texttt{simple\_predictive\_sample($M_c, X_L,X_D,Y,Q$)} $\rightarrow x$. $Y$ is a list of conditions (indices paired with values) on a single row. 
$Q$ is a list of variable indices to simultaneously sample given the conditions are true.
\footnote{Note that $q$ is overloaded, representing both the indices of the queried variable and the variable.}
The output is a list sampled values $x$,
$$
x \sim Pr[Q|X_L,Y].
$$

% SimplePredictiveProbability
\item \texttt{simple\_predictive\_probability($M_c, X_L,X_D,Y,Q,n$)} $\rightarrow p$. $Y$ is a list of conditioning conditions (indices paired with values) on a single row, while $Q$ is a list of queried conditions (indices paired with values) to be simultaneously satisfied given the conditions are true.  $n$ is the number of samples used to approximate the predictive probability.  The output, $p$, is the marginal predictive probability of the queried $Q$ given the conditions $Y$, assuming the model in $X_L$,
$$
p = Pr[Q | X_L,Y].
$$

% Impute
\item \texttt{impute($M_c, X_L,X_D,Y,q,n$) } $\rightarrow e$. The output is a list of the expected values of the posterior predictive distribution for variables corresponding to the indices $q$, approximated using $n$ samples,
$$
e = E_{\textrm{Pr}[q | X_L,Y]}[q].
$$

\end{itemize}

\subsection{Statistics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pseudocode for implementing queries with CrossCat}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The notation in this section is derived from Mansinghka, Shafto, et al, ``CrossCat: A fully Bayesian Nonparametric Method for Analyzing Heterogeneous, High Dimensional Data''. 

\subsection{Prediction}

\begin{itemize}
\item $M_c$ is the column metadata. This includes the model types for the data in each column.
\item $X_L$ is the model, including the prior parameters $\alpha$, the CRP priors, $\vec{\lambda}$, the hyperparameters for the likelihood models, and $\vec{\psi}$ and $\vec{\tau}$ the sufficient statistics for the CRPs and likelihoods. Let $\alpha_D$ represent the CRP prior on views and $\alpha_v$ represent the CRP prior on the categories in view $v$. 
Let $\vec{\lambda_d}$ represent the hyperparameters for likelihood model for dimension $d$. 
Let $\vec{\psi^v}$ represent the sufficient statistics for view $v$ (the count) and $\vec{\psi^v_c}$ represent the sufficient statistics for category $c$ in view $v$ (the count).
Let $\vec{\tau^d_c}$ represent the sufficient statistics summarizing the data from dimension $d$ in category $c$.

\item $X_D$ is the mapping between the model and the data, including $z$ and $y$. Let $z_d$ represent the assigned view for dimension $d$ and $y^v_r$ represent the category for row $r$ in view $v$.

\item $Y$ is a list of conditions, specified as indices $(r,d)$ paired with values $v$. Each pair in this list, specifies an entry for a previously unobserved value in the table, including potentially novel rows and dimensions, $T_{r,d}=v$. 

\item $q$ is a list of queried variables, specified as indices. 

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%
% Begin SimplePredictiveSample
%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[h]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L, X_D, Y, queries$.}
\Output{A list of sampled values $x$.}
\BlankLine
\ForEach{$query \in queries$}{

Extract the queried row and dimension:\\
$row,column \leftarrow query$

Find all elements of $Y$ that apply to the same column:
\[Y' \leftarrow Y s.t. \; Y_{*,2}=column\]

Sample a view and category, allowing the possibility of choosing a new view and/or category: \[
Pr(v', y'| X_L, Y') \propto Pr(Y'|v', y',X_L)Pr(v',y'|X_L)
\] 
The assignment of views is deterministic in the case where $column$ is previously observed,
$v' \leftarrow z_d$. The assignment of categories is deterministic when $v'$ and $row$ are previously observed, 
$y' \leftarrow y^{v'}_r$.

Sample a value from the appropriate posterior predictive (PP) distribution: \[ 
x_i \sim  PP(\vec{\tau^{d}_c}) 
\] 
}

\caption{\texttt{simple\_predictive\_sample}}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%
% Begin SimplePredictiveProbability
%%%%%%%%%%%%%%%%%%%%%

% CONTINUOUS VERSION
\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L, X_D, Y, Q, n$.}
\Output{A list of marginal probabilities $p$.}
\BlankLine

\ForEach{$i \in Q$}{

Extract the queried row and dimension:\\
$r,d,x \leftarrow Q_i$

Find all elements of $Y$ that apply to the same dimension:
\[Y' \leftarrow Y s.t. \; Y_{*,2}=d\]

$p_i \leftarrow 0$

\For{n iterations}{
Sample a view and category, allowing the possibility of choosing a new view and/or category: \[
Pr(v', y'| X_L, Y') \propto Pr(Y'|v', y',X_L)Pr(v',y'|X_L)
\] 
%The assignment of views is deterministic in the case where $d$ is previously observed,
%$v' \leftarrow z_d$. The assignment of categories is deterministic when $v'$ and $r$ are previously observed, 
%$y' \leftarrow y^{v'}_r$.

Assess probability based on the appropriate posterior predictive (PP) distribution: \[ 
p_i =  p_i + PP(Q_i | \vec{\tau^{d}_c}) 
\] 
}

$p_i \leftarrow \frac{p_i}{n}$

}

% DISCRETE APPROXIMATION VERSION
%\begin{algorithm}[ht]
%\DontPrintSemicolon
%\LinesNumbered
%\Indp
%
%\SetKwInOut{Input}{input}
%\SetKwInOut{Output}{output}
%\SetKwInOut{Initialize}{initialize}
%
%\Input{$X_L$, $X_D$, $Y$, $Q$, $n$.}
%\Output{A list of marginal probabilities $p$.}
%\BlankLine
%
%\For{n iterations}{
%Sample from posterior predictive distribution: \\
%$x_i \leftarrow$ \texttt{simple\_predictive\_sample($M_c, X_L,X_D,Y',Q$)}.
%}
%
% NOTE: law of the iterated relevant for choosing the number of bins? sqrt(n*log(log(n)))
%            : typically sqrt(n) is used (per wiki page on histograms)
%Bin values of $x$, identifying the sets that are within $\epsilon$ of the queried values $u$: \\
%$A = \{ x | u - \epsilon < x < u + \epsilon \}$
%
%Approximate probability via histogram: \\
%$p \approx \frac{\sum_{x_i} \mathcal{I_A}}{n}$

\caption{\texttt{simple\_predictive\_probability}}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%
% Begin Impute
%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[ht]
\DontPrintSemicolon
\LinesNumbered
\Indp

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKwInOut{Initialize}{initialize}

\Input{$M_c, X_L, X_D, Y, q, n$.}
\Output{A list of estimated expected values $e$.}
\BlankLine

\For{n iterations}{

Draw sample for each $i \in q$: \\
$x_j =$ \texttt{simple\_predictive\_sample($M_c, X_L,X_D,Y,q$)}

}

Compute the expected values: \\
$e \leftarrow \frac{\sum_j x}{n}$

\caption{\texttt{impute}}
\end{algorithm}

\clearpage
\subsection{Statistics}

\clearpage 
\section{JSON specification for predictive databases}

\subsection{High-level specs for predictive DB interface}

The section provides a JSON schema for $M_c$ the column metadata, $M_r$ the row metadata, 
$X_L$ the latent state for the Predictive Database, and $X_D$ the mapping between the model and the data. 

$M_c$ is a list, where each element is a dict containing four keys: \texttt{name\_to\_idx}, \texttt{idx\_to\_name}, and \texttt{column\_metadata}.  
%
\texttt{name\_to\_idx} and \texttt{idx\_to\_name} convert from column names to indices and indices to 
column names, respectively. 
%
\texttt{column\_metadata} is an array, where each entry is a dict with three keys: \texttt{modeltype}, \texttt{value\_to\_code}, and \texttt{code\_to\_value}. \texttt{modeltype} maps to a string containing one of five possible model types: `asymmetric\_beta\_bernoulli', `normal\_inverse\_gamma', `pitmanyor\_atom', `symmetric\_dirichlet\_discrete', `poisson\_gamma'.
\texttt{value\_to\_code} maps between values and how they are encoded in the data. For instance, for the `binary' data type, this might map `True' to 1 and `False' to 0. For the `categorical' data types, this would function similarly, but with more possible values. For the `normal\_inverse\_gamma' and `poisson\_gamma' model types, \texttt{valueConverter} is empty because there is no conversion to be done. \texttt{code\_to\_value} maps back from codes to values. 

%Finally, the \texttt{likelihoodModel} key contains one of the following strings: `betaBernoilli', `multinomialDirichlet', or `normalInverseChisquared'. The choice of likelihood model is determined by the data type.
 
$M_r$ is an array, where entries are dicts with two keys: \texttt{name\_to\_idx} and \texttt{idx\_to\_name}. These map from row names to row indices and back. 

For instance, consider the following data table: 
\begin{table}[h!]
\begin{center}
\begin{tabular}{c c c c c}
		& Height(in)		& Gender		& Nationality	& IQ  \\
Bob		&	66			& Male		& US			& 105 \\
Steve	& ?				& Male		& Canadian	& 100 \\ 
Jill		& 60				& Female		& US			& 104 \\
\end{tabular}
\end{center}
\end{table}
\FloatBarrier

$M_c$ and $M_r$ could be:
\begin{verbatim}
M_c: {
  Òname_to_idxÓ: {0: ÒHeightÓ, 1: ÒGenderÓ, 2: ÒNationalityÓ, 3: ÒIQÓ}
  Òidx_to_nameÓ: {ÒHeightÓ:0, ÒGenderÓ: 1, ÒNationalityÓ: 2, ÒIQÓ: 3}
  Òcolumn_metadataÓ: [
    {ÒmodeltypeÓ: Ònormal_inverse_gammaÓ, Òvalue_to_codeÓ: {}, Òcode_to_valueÓ: {}},
    {ÒmodeltypeÓ: Òasymmetric_beta_bernoulliÓ, Òvalue_to_codeÓ: {0:ÓMaleÓ, 1:ÓFemaleÓ}, ...
    	Òcode_to_valueÓ: {ÒMaleÓ:0, ÒFemaleÓ:1}},
    {ÒmodeltypeÓ: Òsymmetric_dirichlet_discreteÓ, Òcode_to_valueÓ: {ÒUSÓ:0, ÒCanadianÓ:1}, ...
    	Òvalue_to_codeÓ: {1:ÓCanadianÓ, 0:ÓUSÓ}},
    {ÒmodeltypeÓ: Ònormal_inverse_gammaÓ, Òvalue_to_codeÓ: {}, Òcode_to_valueÓ:{}}
    ]
}

M_r: {
  Òname_to_idxÓ: {ÒBobÓ:0, ÒSteveÓ:1, ÒJillÓ:2},
  Òidx_to_nameÓ: {0:ÓBobÓ, 1:ÓSteveÓ, 3:ÓJillÓ},
}
\end{verbatim}

The latent state, $X_L$, is a dict with three keys:  
\texttt{column\_partition},
\texttt{column\_hypers}, and
\texttt{view\_state}.
%
\texttt{column\_partition} is a dict with three keys: \texttt{hypers}, \texttt{assignments}, and \texttt{counts}.
\texttt{hypers} is a dict containing a single key, \texttt{alpha}, which maps to a number representing the concentration parameter of the CRP. \texttt{assignments} is an array, where each element is an integer representing the view to which that column is assigned. \texttt{counts} is an array with length equal to the number of views, where each entry is the number of columns in that view. 

\texttt{column\_hypers} is an array where each element is a dict whose structure depends on the \texttt{modeltype} of that column. For instance, for a column with a `asymmetric\_beta\_bernoulli' modeltype, the dict would have three keys: \texttt{log\_strength} and\texttt{balance}. One additional key would apply to all model types. \texttt{fixed} would map to the strings `true' or `false' to indicate whether the parameters were fixed or not.

\texttt{view\_state} is an array where each element is a a dict with three keys: \texttt{column\_names}, \texttt{row\_partition\_model} and \texttt{column\_component\_suffstats}. 
\texttt{column\_names} is an array of strings where the indices are locally consistent (within the view).

\texttt{row\_partition\_model} is a dict with two keys: \texttt{hypers} and \texttt{counts}. \texttt{hypers} is a dict with a single key, \texttt{alpha}, which maps to a number representing the concentration parameter for the row CRP in that view. \texttt{counts} is an array, with length equal to the number of categories, where each element is the number of rows assigned to that category.
%
\texttt{column\_component\_suffstats} is an array of arrays of dicts, where the structure of each dict depends on the model type. The outer array is over the columns while the inner array is over categories. Each dict contains the sufficient statistics that are appropriate for the model type of the column.  For instance, for an `asymmetric\_beta\_bernoulli' column, the dict would have three keys: \texttt{0\_count}, \texttt{1\_count}, and \texttt{N}.

Assume the underlying crosscat state is:
\begin{verbatim}
views: [ 1 1 2 3]
categories: [
	[1 1 2]
	[1 2 1]
	[1 1 1]
]
\end{verbatim}

Then, $X_L$ could be: 
\begin{verbatim}
X_L: {
  Òcolumn_partitionÓ: {
	ÒhypersÓ: {Òlog_alphaÓ: 1.1},
	ÒassignmentsÓ: [0, 0, 1, 2],
	ÒcountsÓ: [2, 1, 1],
},
  Òcolumn_hypersÓ: [
	{ÒfixedÓ: false, ÒmuÓ: 63.5, Ólog_kappaÓ: 8.2, Ólog_alphaÓ: 2.3, Òlog_betaÓ: 3.4},
	{ÒfixedÓ: false, Òlog_strengthÓ: 10, ÒbalanceÓ: 1.2}
	{ÒfixedÓ: true, Òlog_alphaÓ: .5, ÒKÓ: 2},
	{ÒfixedÓ: false, ÒmuÓ: 101.1, Ólog_kappaÓ: 4, Òlog_alphaÓ: 10.1, Òlog_betaÓ: 5}
	],
  Òview_stateÓ: [
	{Òrow_partition_modelÓ: {ÒhypersÓ: {Òlog_alphaÓ:3.2}, ÒcountsÓ:[2, 1]}, ...
		"column_names": ["Height(in)", "Gender"], ...
		Òcolumn_component_suffstatsÓ: [[{Òsum_xÓ: 66.0, Òsum_x_sqÓ: 4356.0},...
		{Òsum_xÓ: 60.0, Òsum_x_sqÓ: 3600.0}], [{Ò0_countÓ: 2, Ò1_countÓ: 0}, ...
			{Ò0_countÓ: 0, Ò1_countÓ: 1}]]},
	{Òrow_partition_modelÓ: {ÒhypersÓ: {Òlog_alphaÓ:2.2}, ÒcountsÓ:[2, 1]}, ...
		"column_names": ["Nationality"], ...
		Òcolumn_component_suffstatsÓ: [[{Ò0_countÓ: 2, Ò1_countÓ:0},{Ò0_countÓ: 2, Ò1_countÓ:0}]]},
	{Òrow_partition_modelÓ: {ÒhypersÓ: {Òlog_alphaÓ:0.8}, ÒcountsÓ:[3]}, ...
		"column_names": ["IQ"], ...
		Òcolumn_component_suffstatsÓ: [[{Òsum_xÓ: 309.0, Òsum_x_sqÓ: 31481.0}]]}
	]
}
\end{verbatim}

The mapping between the latent state and the data, $X_D$, is an array of arrays. The elements of the outer array correspond to views and the inner array corresponds to the rows. Entries are integers indicating, for a row in a view, which category it belongs to.

\begin{verbatim}
X_D: [
	[0, 0, 1],
	[0, 1, 0],
	[0, 0, 0]
]
\end{verbatim}

\end{document} 
